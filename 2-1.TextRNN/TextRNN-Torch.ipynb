{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextRNN_pytorch.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"RvHvrfnVFhdf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":161},"outputId":"045e853d-b2cb-41b4-9b1c-3a4bc88cf618","executionInfo":{"status":"ok","timestamp":1550468983013,"user_tz":-540,"elapsed":5590,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","  reference: \n","'''\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","dtype = torch.FloatTensor\n","\n","sentences = [ \"i like dog\", \"i love coffee\", \"i hate milk\"]\n","\n","word_list = \" \".join(sentences).split()\n","word_list = list(set(word_list))\n","word_dict = {w: i for i, w in enumerate(word_list)}\n","number_dict = {i: w for i, w in enumerate(word_list)}\n","n_class = len(word_dict)\n","\n","# TextRNN Parameter\n","batch_size = len(sentences)\n","n_hidden = 5 # number of hidden units in one cell\n","\n","def make_batch(sentences):\n","    input_batch = []\n","    target_batch = []\n","\n","    for sen in sentences:\n","        word = sen.split()\n","        input = [word_dict[n] for n in word[:-1]]\n","        target = word_dict[word[-1]]\n","\n","        input_batch.append(np.eye(n_class)[input])\n","        target_batch.append(target)\n","\n","    return input_batch, target_batch\n","\n","# to Torch.Tensor\n","input_batch, target_batch = make_batch(sentences)\n","input_batch = Variable(torch.Tensor(input_batch))\n","target_batch = Variable(torch.LongTensor(target_batch))\n","\n","class TextRNN(nn.Module):\n","    def __init__(self, n_hidden, n_class):\n","        super(TextRNN, self).__init__()\n","        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden)\n","        self.linear = nn.Linear(n_hidden, n_class)\n","\n","    def forward(self, X):\n","        X = X.transpose(0, 1) # X : [n_step, batch_size, n_class]\n","        outputs, hidden = self.rnn(X)\n","        # outputs : [n_step, batch_size, num_directions(=1) * n_hidden]\n","        # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n","        hidden = hidden.squeeze(0)\n","        logits = self.linear(hidden)\n","        return logits\n","\n","model = TextRNN(n_hidden, n_class)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training\n","for epoch in range(5000):\n","    optimizer.zero_grad()\n","    \n","    # input_batch : [batch_size, n_step, n_class]\n","    output = model(input_batch)\n","\n","    # output : [batch_size, n_class], target_batch : [batch_size] (LongTensor, not one-hot)\n","    loss = criterion(output, target_batch)\n","    if (epoch + 1) % 1000 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","input = [sen.split()[:2] for sen in sentences]\n","\n","# Predict\n","hidden = Variable(torch.zeros(1, batch_size, n_hidden))\n","predictions = model(input_batch).data.max(1, keepdim=True)[1].squeeze().numpy()\n","\n","for i, sentence in enumerate(sentences):\n","    print(sentence.split()[:2], '->', number_dict[predictions[i]])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Epoch: 1000 cost = 0.074515\n","Epoch: 2000 cost = 0.017102\n","Epoch: 3000 cost = 0.006943\n","Epoch: 4000 cost = 0.003426\n","Epoch: 5000 cost = 0.001850\n","['i', 'like'] -> dog\n","['i', 'love'] -> coffee\n","['i', 'hate'] -> milk\n"],"name":"stdout"}]}]}