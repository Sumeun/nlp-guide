{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT-Torch.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"AxkvIhfd61hP","colab_type":"code","outputId":"716222c5-62df-45b6-d0d6-e661c422775c","executionInfo":{"status":"ok","timestamp":1550982389475,"user_tz":-540,"elapsed":6364,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}},"colab":{"base_uri":"https://localhost:8080/","height":390}},"cell_type":"code","source":["# install pretrained BERT model\n","!pip3 install pytorch_pretrained_bert"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pytorch_pretrained_bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n","\u001b[K    100% |████████████████████████████████| 122kB 3.8MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.0.1.post2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2018.1.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.9.95)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.18.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.6)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.95 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.12.95)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.3)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.0)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.22)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2018.11.29)\n","Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.95->boto3->pytorch_pretrained_bert) (0.14)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.95->boto3->pytorch_pretrained_bert) (2.5.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.95->boto3->pytorch_pretrained_bert) (1.11.0)\n","Installing collected packages: pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.1\n"],"name":"stdout"}]},{"metadata":{"id":"nlPKIeqtJ40Y","colab_type":"code","outputId":"659b9977-2cfc-4ffe-b665-7546506fd4ed","executionInfo":{"status":"ok","timestamp":1550986886786,"user_tz":-540,"elapsed":44490,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","  \n","'''\n","\n","# Sentiment Analysis\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from pytorch_pretrained_bert import BertModel, BertTokenizer, BertAdam\n","from copy import deepcopy\n","\n","from torch import LongTensor as LT\n","\n","# words sentences\n","sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n","targets = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is bad.\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","inputs = []\n","for sent in sentences:\n","    tokens =  ['[CLS]'] + tokenizer.tokenize(sent) + ['[SEP]']\n","    sequence = tokenizer.convert_tokens_to_ids(tokens)\n","    inputs.append(sequence)\n","\n","input_batch = LT(inputs)\n","target_batch = LT(targets)\n","\n","input_dims = 768\n","output_dims = 2\n","\n","BERT_model = BertModel.from_pretrained('bert-base-uncased')\n","\n","\n","class BERTEncoder(nn.Module):\n","    def __init__(self):\n","        super(BERTEncoder, self).__init__()\n","        self.encoder = deepcopy(BERT_model)\n","\n","    def forward(self, x, token_id=None, mask=None):\n","        _, feat = self.encoder(x, token_type_ids=token_id, attention_mask=mask, output_all_encoded_layers=False)\n","        return feat\n","\n","\n","class BERTClassifier(nn.Module):\n","    def __init__(self, input_dims, output_dims):\n","        super(BERTClassifier, self).__init__()\n","        self.dropout = nn.Dropout(p = 0.1)\n","        self.classifier = nn.Linear(input_dims, output_dims)\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        out = self.classifier(x)\n","        return out\n","\n","class BERTForSequenceClassifier(nn.Module):\n","    def __init__(self, input_dims, output_dims):\n","        super(BERTForSequenceClassifier, self).__init__()\n","        self.encoder = BERTEncoder()\n","        self.classifier = BERTClassifier(input_dims, output_dims)\n","    \n","    def forward(self, x):\n","        x = self.encoder(x)\n","        out = self.classifier(x)\n","        return out\n","        \n","        \n","model = BERTForSequenceClassifier(input_dims, output_dims)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = BertAdam(model.parameters(), lr=5e-5)\n","\n","# Training\n","model.train()\n","for epoch in range(15):\n","    optimizer.zero_grad()\n","    output = model(input_batch)\n","\n","    # output : [batch_size, num_classes], target_batch : [batch_size] (LongTensor, not one-hot)\n","    loss = criterion(output, target_batch)\n","    if (epoch + 1) % 3 == 0:\n","        print('Epoch:', '%02d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","    loss.backward()\n","    optimizer.step()\n","    \n","# Test\n","test_text = ['sorry hate you', 'you love me']\n","tests = []\n","for sent in test_text:\n","    tokens =  ['[CLS]'] + tokenizer.tokenize(sent) + ['[SEP]']\n","    sequence = tokenizer.convert_tokens_to_ids(tokens)\n","    tests.append(sequence)\n","\n","test_batch = LT(tests)\n","\n","# Predict\n","model.eval()\n","result = model(test_batch).data.max(1)[1]\n","for i, text in enumerate(test_text):\n","  if result[i] == 1:\n","      print(\"\\'\" + text + \"\\'\", \"is good :)\")\n","  else:\n","      print(\"\\'\"+ text + \"\\'\", \"is bad :(\")"],"execution_count":57,"outputs":[{"output_type":"stream","text":["Epoch: 03 cost = 0.336113\n","Epoch: 06 cost = 0.076280\n","Epoch: 09 cost = 0.005214\n","Epoch: 12 cost = 0.000949\n","Epoch: 15 cost = 0.000777\n","'sorry hate you' is bad :(\n","'you love me' is good :)\n"],"name":"stdout"}]},{"metadata":{"id":"DzfFWa3R-Rf-","colab_type":"code","outputId":"61e4ef10-aa50-4ab9-a4ab-f7098c1608da","executionInfo":{"status":"ok","timestamp":1550987230886,"user_tz":-540,"elapsed":118082,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"cell_type":"code","source":["# Next Sentence Prediction\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from termcolor import colored\n","from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\n","\n","from torch import LongTensor as LT\n","\n","\n","# words sentences\n","next_sentences = [\"Once upon a time, there was a kind girl named Cinderella.\",\n","                  \"All of the animals loved her, especially two mice named Gus and Jaq.\", \n","                  \"They'd do anything for the girl they called Cinderelly.\",\n","                  \"Cinderella lived with her stepmother and her two stepsisters, Anastasia and Drizella.\",\n","                  \"They were very mean to Cinderella, making her work all day cleaning, sewing, and cooking.\",\n","                  \"She tried her best to make them happy.\",\n","                  \"Cinderella's stepmother, Lady Tremaine, was cold, cruel, and jealous of Cinderella’s charm and beauty.\"]\n","\n","not_next_sentences = [\"One day, a messenger arrived with a special invitation.\",\n","                      \"Lady Tremaine didn't want Cinderella to go to the ball.\",\n","                      \"She wanted the Prince to meet Anastasia and Drizella.\",\n","                      \"It was a bit old-fashioned, but Cinderella could make it beautiful!\",\n","                      \"Cinderella was overjoyed when she saw the dress.\",\n","                      \"They ripped the dress and pulled off the beads.\",\n","                      \"Suddenly, her fairy godmother appeared.\",\n","                      \"Lady Tremaine didn't stop them.\",\n","                      \"Cinderella's dream of going to the ball was through.\",\n","                      \"At the ball, Prince Charming couldn't take his eyes off Cinderella.\",\n","                      \"The orchestra played, and the Prince began to dance with the wonderful girl whose name he still didn't know.\",\n","                      \"When the clock struck midnight, the magic spell would wear off!\"]\n","\n","targets = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]  # 1 is Next, 0 is Not Next.\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","inputs = []\n","attn_masks = []\n","segment_ids = []\n","maxlen = 50\n","\n","for i in range(len(next_sentences) - 1):\n","    token_a = ['[CLS]'] + tokenizer.tokenize(next_sentences[i]) + ['[SEP]']\n","    token_b = tokenizer.tokenize(next_sentences[i+1]) + ['[SEP]']\n","    tokens =  token_a + token_b\n","    sequence = tokenizer.convert_tokens_to_ids(tokens)\n","    seqlen = len(sequence)\n","    padding = [0] * (maxlen - seqlen)\n","    sequence += padding\n","    attn_mask = [1] * seqlen + padding\n","    segment_id = [0] * len(token_a) + [1] * len(token_b) + padding\n","    inputs.append(sequence)\n","    attn_masks.append(attn_mask)\n","    segment_ids.append(segment_id)\n","\n","for i in range(0, len(not_next_sentences) - 1, 2):\n","    token_a = ['[CLS]'] + tokenizer.tokenize(not_next_sentences[i]) + ['[SEP]']\n","    token_b = tokenizer.tokenize(not_next_sentences[i+1]) + ['[SEP]']\n","    tokens =  token_a + token_b\n","    sequence = tokenizer.convert_tokens_to_ids(tokens)\n","    seqlen = len(sequence)\n","    padding = [0] * (maxlen - seqlen)\n","    sequence += padding\n","    attn_mask = [1] * seqlen + padding\n","    segment_id = [0] * len(token_a) + [1] * len(token_b) + padding\n","    inputs.append(sequence)\n","    attn_masks.append(attn_mask)\n","    segment_ids.append(segment_id)\n","    \n","input_batch = LT(inputs)\n","target_batch = LT(targets)\n","attn_masks = LT(attn_masks)\n","segment_ids = LT(segment_ids)\n","\n","input_dims = 768\n","output_dims = 2\n","\n","\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = output_dims)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = BertAdam(model.parameters(), lr=5e-5)\n","\n","# Training\n","model.train()\n","for epoch in range(15):\n","    optimizer.zero_grad()\n","    loss = model(input_batch, segment_ids, attn_masks, target_batch)\n","    if (epoch + 1) % 3 == 0:\n","        print('Epoch:', '%02d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","    loss.backward()\n","    optimizer.step()\n","    \n","# Test\n","test_text = [\"Once upon a time, there was a kind girl named Cinderella.\",\n","             \"All of the animals loved her, especially two mice named Gus and Jaq.\",\n","             \"When the clock struck midnight, the magic spell would wear off!\"]\n","\n","tests = []\n","test_attn_masks = []\n","test_segment_ids = []\n","\n","for i in range(len(test_text) - 1):\n","    token_a = ['[CLS]'] + tokenizer.tokenize(test_text[i]) + ['[SEP]']\n","    token_b = tokenizer.tokenize(test_text[i+1]) + ['[SEP]']\n","    tokens =  token_a + token_b\n","    sequence = tokenizer.convert_tokens_to_ids(tokens)\n","    seqlen = len(sequence)\n","    padding = [0] * (maxlen - seqlen)\n","    sequence += padding\n","    attn_mask = [1] * seqlen + padding\n","    segment_id = [0] * len(token_a) + [1] * len(token_b) + padding\n","    tests.append(sequence)\n","    test_attn_masks.append(attn_mask)\n","    test_segment_ids.append(segment_id)\n","\n","test_batch = LT(tests)\n","test_segment_ids = LT(test_segment_ids)\n","test_attn_masks = LT(test_attn_masks)\n","\n","# Predict\n","model.eval()\n","result = model(test_batch, test_segment_ids, test_attn_masks).data.max(1)[1]\n","for i in range(len(test_text) - 1):\n","  if result[i] == 1:\n","      print(\"\\'\" + test_text[i+1] + \"\\' is\", colored('next sentence', 'blue'), \"of \\'\" + test_text[i] + \"\\'\")\n","  else:\n","      print(\"\\'\" + test_text[i+1] + \"\\' is\", colored('not next sentence', 'red'), \"of \\'\" + test_text[i] + \"\\'\")"],"execution_count":59,"outputs":[{"output_type":"stream","text":["Epoch: 03 cost = 0.382100\n","Epoch: 06 cost = 0.068060\n","Epoch: 09 cost = 0.005214\n","Epoch: 12 cost = 0.000469\n","Epoch: 15 cost = 0.000347\n","'All of the animals loved her, especially two mice named Gus and Jaq.' is \u001b[34mnext sentence\u001b[0m of 'Once upon a time, there was a kind girl named Cinderella.'\n","'When the clock struck midnight, the magic spell would wear off!' is \u001b[31mnot next sentence\u001b[0m of 'All of the animals loved her, especially two mice named Gus and Jaq.'\n"],"name":"stdout"}]},{"metadata":{"id":"7x6Q1TDxvYG4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"outputId":"811ff260-05f6-4875-fa84-9d3474f527bc","executionInfo":{"status":"ok","timestamp":1550986799155,"user_tz":-540,"elapsed":71316,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}}},"cell_type":"code","source":["# Question Answering\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from termcolor import colored\n","from pytorch_pretrained_bert import BertTokenizer, BertForQuestionAnswering, BertAdam\n","\n","from torch import LongTensor as LT\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# example sentences\n","context_sentences = ['I had a meal at the indian restaurant today.',\n","                     'he had a wedding ceremony yesterday.',\n","                     'she will go to a graduation ceremony tomorrow.']\n","\n","question_sentences = ['where did i have a meal today?',\n","                      'what did he do yesterday?',\n","                      'where will she go tomorrow?']\n","\n","input_ids = []\n","input_mask = []\n","segment_ids = []\n","maxlen = 80\n","\n","for context, question in zip(context_sentences, question_sentences):\n","  tokens =  ['[CLS]'] + tokenizer.tokenize(question) + ['[SEP]'] + tokenizer.tokenize(context) + ['[SEP]']\n","  sequence = tokenizer.convert_tokens_to_ids(tokens)\n","  padding = [0] * (maxlen - len(tokens))\n","  attn_mask = [1] * len(sequence) + padding\n","  sequence += padding\n","  segment_id = [0] * len(sequence)\n","  input_ids.append(sequence)\n","  input_mask.append(attn_mask)\n","  segment_ids.append(segment_id)\n","  \n","start_positions = [16, 11, 13]\n","end_positions = [18, 13, 15]\n","\n","input_ids = LT(input_ids)\n","input_mask = LT(input_mask)\n","segment_ids = LT(segment_ids)\n","start_positions = LT(start_positions)\n","end_positions = LT(end_positions)\n","\n","model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n","criterion = nn.CrossEntropyLoss()\n","optimizer = BertAdam(model.parameters(), lr=5e-5)\n","\n","# Training\n","model.train()\n","for epoch in range(15):\n","    optimizer.zero_grad()\n","    loss = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n","\n","    if (epoch + 1) % 3 == 0:\n","        print('Epoch:', '%02d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","    loss.backward()\n","    optimizer.step()\n","    \n","def tokens_to_sentence(input_id, start_position, end_position):\n","    tokens = tokenizer.convert_ids_to_tokens(input_id[start_position:end_position])\n","    sentence = ''\n","    for token in tokens:\n","        if token[:2] == '##':\n","            sentence += token[2:]\n","        elif len(sentence) == 0:\n","            sentence += token\n","        else:\n","          sentence += ' ' + token\n","          \n","    return sentence\n","\n","# Prediction\n","model.eval()\n","(start_predictions, end_predictions) = model(input_ids, segment_ids, input_mask)\n","start_predictions, end_predictions = start_predictions.data.max(1)[1], end_predictions.data.max(1)[1]\n","\n","print('\\n[train predictions]')\n","for i, (cs, qs) in enumerate(zip(context_sentences, question_sentences)):\n","    print('Context:', cs)\n","    print('Question:', qs)\n","    sentence = tokens_to_sentence(input_ids[i].numpy(), start_predictions[i].numpy(), end_predictions[i].numpy())\n","    print('Answer:', sentence)"],"execution_count":55,"outputs":[{"output_type":"stream","text":["Epoch: 03 cost = 0.895173\n","Epoch: 06 cost = 0.026652\n","Epoch: 09 cost = 0.005986\n","Epoch: 12 cost = 0.752293\n","Epoch: 15 cost = 0.025888\n","\n","[train predictions]\n","Context: I had a meal at the indian restaurant today.\n","Question: where did i have a meal today?\n","Answer: indian restaurant\n","Context: he had a wedding ceremony yesterday.\n","Question: what did he do yesterday?\n","Answer: wedding ceremony\n","Context: she will go to a graduation ceremony tomorrow.\n","Question: where will she go tomorrow?\n","Answer: graduation ceremony\n"],"name":"stdout"}]}]}