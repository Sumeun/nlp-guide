{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextCNN_tensor.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"NZnKV5Pae4zu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":377},"outputId":"7b2c6f62-fb8a-4335-e094-e918c4544382","executionInfo":{"status":"ok","timestamp":1550463380542,"user_tz":-540,"elapsed":22807,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","  Reference : https://github.com/ioatr/textcnn\n","'''\n","import tensorflow as tf\n","import numpy as np\n","\n","tf.reset_default_graph()\n","\n","# Text-CNN Parameter\n","embedding_size = 2 # n-gram\n","sequence_length = 3\n","num_classes = 2 # 0 or 1\n","filter_sizes = [2,2,2] # n-gram window\n","num_filters = 3\n","\n","# 3 words sentences (=sequence_length is 3)\n","sentences = [\"i love you\",\"he loves me\", \"she likes baseball\", \"i hate you\",\"sorry for that\", \"this is awful\"]\n","labels = [1,1,1,0,0,0] # 1 is good, 0 is not good.\n","\n","word_list = \" \".join(sentences).split()\n","word_list = list(set(word_list))\n","word_dict = {w: i for i, w in enumerate(word_list)}\n","vocab_size = len(word_dict)\n","\n","inputs = []\n","for sen in sentences:\n","    inputs.append(np.asarray([word_dict[n] for n in sen.split()]))\n","\n","outputs = []\n","for out in labels:\n","    outputs.append(np.eye(num_classes)[out]) # ONE-HOT : To using Tensor Softmax Loss function\n","\n","# Model\n","class TextCNN(object):\n","    def __init__(self, sess, vocab_size, sequence_length, embedding_size, \n","                 filter_sizes, num_filters, num_classes):\n","        self.sess = sess\n","        self.vocab_size = vocab_size\n","        self.sequence_length = sequence_length\n","        self.embedding_size = embedding_size\n","        self.filter_sizes = filter_sizes\n","        self.num_filters = num_filters\n","        self.num_classes = num_classes\n","        self._build_model()\n","      \n","    def _build_model(self):\n","        # Placeholders for input, output\n","        with tf.variable_scope(\"placeholder\"):\n","            self.X = tf.placeholder(tf.int32, [None, self.sequence_length])\n","            self.Y = tf.placeholder(tf.int32, [None, self.num_classes])\n","            self.embbeding_placeholder = tf.placeholder(tf.float32, (self.vocab_size, self.embedding_size))\n","      \n","        # Embedding layer\n","        with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n","            W = tf.get_variable(\"embedding_W\", dtype=tf.float32, initializer=tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0))\n","            embedded_chars = tf.nn.embedding_lookup(W, self.X)\n","            embedded_chars_expanded = tf.expand_dims(embedded_chars, -1) # add channel(=1) [batch_size, sequence_length, embedding_size, 1]      \n","      \n","        # Create a convolution + maxpool layer for each filter size\n","        pooled_outputs = []\n","        for filter_size in self.filter_sizes:\n","            with tf.variable_scope(\"conv-maxpool-%s\" % filter_size, reuse=tf.AUTO_REUSE):\n","                # Convolution Layer\n","                filter_shape = (filter_size, self.embedding_size, 1, self.num_filters)\n","                W = tf.get_variable(\"W\", dtype=tf.float32, initializer=tf.truncated_normal(filter_shape, stddev=0.1))\n","                b = tf.get_variable(\"b\", dtype=tf.float32, initializer=tf.constant(0.1, shape=(self.num_filters,)))\n","\n","                conv = tf.nn.conv2d(embedded_chars_expanded, # [batch_size, sequence_length, embedding_size, 1]\n","                                    W,              # [filter_size(n-gram window), embedding_size, 1, num_filters(=3)]\n","                                    strides=[1, 1, 1, 1],\n","                                    padding='VALID')\n","                # Apply nonlinearity\n","                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n","                # Maxpooling over the outputs\n","                pooled = tf.nn.max_pool(h,\n","                                        ksize=[1, sequence_length - filter_size + 1, 1, 1], # [batch_size, filter_height, filter_width, channel]\n","                                        strides=[1, 1, 1, 1],\n","                                        padding='VALID')\n","                pooled_outputs.append(pooled) # dim of pooled : [batch_size(=6), output_height(=1), output_width(=1), channel(=1)]\n","\n","        # Combine all the pooled features\n","        num_filters_total = self.num_filters * len(self.filter_sizes)\n","        h_pool = tf.concat(pooled_outputs, self.num_filters) # h_pool : [batch_size(=6), output_height(=1), output_width(=1), channel(=1) * 3]\n","        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total]) # [batch_size, ]\n","\n","        # Final (unnormalized) scores and predictions\n","        with tf.variable_scope(\"output\", reuse=tf.AUTO_REUSE):\n","            W = tf.get_variable('output_W', shape=[num_filters_total, num_classes], \n","                                initializer=tf.contrib.layers.xavier_initializer())\n","            b = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n","            logits = tf.nn.xw_plus_b(h_pool_flat, W, b)  \n","            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=self.Y))\n","            self.optimizer = tf.train.AdamOptimizer(0.001).minimize(self.cost)\n","            hypothesis = tf.nn.softmax(logits)\n","            self.predictions = tf.argmax(hypothesis, 1)\n","\n","        tf.global_variables_initializer().run()\n","    \n","    def train(self, inputs, labels):\n","        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: inputs, self.Y: labels})\n","     \n","    def predict(self, inputs):\n","        return self.sess.run([self.predictions], feed_dict={self.X: inputs})\n","      \n","# Training\n","run_config = tf.ConfigProto()\n","run_config.gpu_options.allow_growth=True\n","with tf.Session(config=run_config) as sess:\n","    model = TextCNN(sess, vocab_size, sequence_length, embedding_size, filter_sizes, num_filters, num_classes)\n","    for epoch in range(5000):\n","        loss, _ = model.train(inputs, outputs)\n","        if (epoch + 1)%1000 == 0:\n","            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","    \n","    # Test\n","    test_text = ['sorry hate you', 'you love me']\n","    tests = []\n","    for text in test_text:\n","        tests.append(np.asarray([word_dict[n] for n in text.split()]))\n","    result = model.predict(tests)[0]\n","    \n","for i, text in enumerate(test_text):\n","  if result[i] == 0:\n","      print(\"\\'\" + text + \"\\'\", \"is bad :(\")\n","  else:\n","      print(\"\\'\"+ text + \"\\'\", \"is good :)\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Epoch: 1000 cost = 0.002293\n","Epoch: 2000 cost = 0.000452\n","Epoch: 3000 cost = 0.000163\n","Epoch: 4000 cost = 0.000073\n","Epoch: 5000 cost = 0.000036\n","'sorry hate you' is bad :(\n","'you love me' is good :)\n"],"name":"stdout"}]}]}