{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment-Tensor.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"raxGxmqe03KX","colab_type":"text"},"cell_type":"markdown","source":["## 1.install konlpy"]},{"metadata":{"id":"KvEZ9P7bvb3G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":431},"outputId":"2d626049-1f93-43f5-8800-043f5f90f0f7","executionInfo":{"status":"ok","timestamp":1550563782917,"user_tz":-540,"elapsed":12440,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}}},"cell_type":"code","source":["!apt-get update\n","!apt-get install g++ openjdk-8-jdk python-dev python3-dev\n","!pip3 install JPype1-py3\n","!pip3 install konlpy\n","!JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.88.161)] [Waiting for headers] [Wa\r0% [1 InRelease gpgv 3,609 B] [Connecting to archive.ubuntu.com (91.189.88.161)\r                                                                               \rHit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n","\r0% [1 InRelease gpgv 3,609 B] [Connecting to archive.ubuntu.com (91.189.88.161)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [1 InRelease gpgv 3,609 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 3,609 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rIgn:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","\r0% [1 InRelease gpgv 3,609 B] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 3,609 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","\r0% [1 InRelease gpgv 3,609 B] [Waiting for headers] [Connecting to ppa.launchpa\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpad.net\r0% [2 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","Hit:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n","Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","python-dev is already the newest version (2.7.15~rc1-1).\n","g++ is already the newest version (4:7.3.0-3ubuntu2.1).\n","python3-dev is already the newest version (3.6.7-1~18.04).\n","openjdk-8-jdk is already the newest version (8u191-b12-2ubuntu0.18.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n","Requirement already satisfied: JPype1-py3 in /usr/local/lib/python3.6/dist-packages (0.5.5.2)\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.1)\n","Requirement already satisfied: JPype1>=0.5.7 in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.6.3)\n"],"name":"stdout"}]},{"metadata":{"id":"NqxaqOld087d","colab_type":"text"},"cell_type":"markdown","source":["## 2.Preprocess Tools"]},{"metadata":{"id":"zY02EJJNv3oA","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import re\n","from collections import Counter\n","from konlpy.tag import Okt\n","import gensim\n","\n","okt = Okt()\n","\n","\n","def morphs_extractor(sentence):\n","    \"\"\"\n","    extract morphs\n","    \"\"\"\n","    tokens = okt.morphs(sentence, norm=True, stem=True)\n","    return tokens\n","        \n","    \n","def morphs_process(lines):\n","    tokens = []\n","    for line in lines:\n","        token = morphs_extractor(line)\n","        tokens.append(token)\n","    return tokens\n","\n","  \n","def sentence_to_index_morphs(lines, vocab, max_length=0):\n","    tokens = []\n","    indexes = []\n","    max_len = max_length\n","\n","    assert (type(lines) is list or tuple), \"Input type must be list or tuple.\"\n","\n","    if max_len == 0:\n","        for line in lines:\n","            token = morphs_extractor(line)\n","            tokens.append(token)\n","            length = len(token)\n","            if max_len < length:\n","                max_len = length\n","    else:\n","        for line in lines:\n","            token = morphs_extractor(line)\n","            tokens.append(token)            \n","\n","    for token in tokens:\n","        if len(token) < max_len:\n","            temp = token\n","            for _ in range(len(temp), max_len):\n","                temp.append('<PAD>')\n","        else:\n","            temp = token[:max_len]\n","        index = []\n","        for char in temp:\n","            if char in vocab.keys():\n","                index.append(vocab[char])\n","            else:\n","                index.append(vocab['<UNK>'])\n","        indexes.append(index)\n","\n","    return indexes\n","\n","  \n","def batch_iter(data, batch_size, num_epochs, shuffle=True):\n","    \"\"\"\n","    Generates a batch iterator for a dataset.\n","    \"\"\"\n","    data = np.array(data)\n","    data_size = len(data)\n","    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n","    for epoch in range(num_epochs):\n","        # Shuffle the data at each epoch\n","        if shuffle:\n","            shuffle_indices = np.random.permutation(np.arange(data_size))\n","            shuffled_data = data[shuffle_indices]\n","        else:\n","            shuffled_data = data\n","        for batch_num in range(num_batches_per_epoch):\n","            start_index = batch_num * batch_size\n","            end_index = min((batch_num + 1) * batch_size, data_size)\n","            yield shuffled_data[start_index:end_index]\n","            \n","            \n","def make_embedding_vectors(data, embedding_size=300):\n","    tokens = morphs_process(data)\n","    wv_model = gensim.models.Word2Vec(min_count=1, window=5, size=embedding_size)\n","    wv_model.build_vocab(tokens)\n","    wv_model.train(tokens, total_examples=wv_model.corpus_count, epochs=wv_model.epochs)\n","    word_vectors = wv_model.wv\n","    \n","    vocab = dict()\n","    vocab['<PAD>'] = 0\n","    vocab['<UNK>'] = 1\n","    idx = 2\n","    for word in word_vectors.vocab:\n","        vocab[word] = idx\n","        idx += 1\n","        \n","    embedding = []\n","    embedding.append(np.random.normal(size=300))\n","    embedding.append(np.random.normal(size=300))\n","    for word in word_vectors.vocab:\n","        embedding.append(word_vectors[word])\n","    embedding = np.asarray(embedding)\n","    vocab_size = len(embedding)\n","    \n","    return embedding, vocab, vocab_size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j5JFFpc20_b1","colab_type":"text"},"cell_type":"markdown","source":["## 3.Build Model"]},{"metadata":{"id":"Fpmr6_NRwOag","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","\n","class TextCNN(object):\n","    \"\"\"\n","    The implementation is based on following:\n","    dennybritz: simplified implementation of Kim's Convolutional Neural Networks for Sentence Classification paper in Tensorflow.\n","    \"\"\"\n","\n","    def __init__(\n","            self, sess, vocab_size, sequence_length=30, embedding_size=300,\n","            filter_sizes=(3, 4, 5), num_filters=128, n_class=2, lr=1e-2, trainable=True):\n","        self.sess = sess\n","        self.vocab_size = vocab_size\n","        self.sequence_length = sequence_length\n","        self.embedding_size = embedding_size\n","        self.filter_sizes = filter_sizes\n","        self.num_filters = num_filters\n","        self.n_class = n_class\n","        self.lr = lr\n","        self.trainable = trainable\n","        self._build_net()\n","\n","    def _build_net(self):\n","        # Placeholders for input, output\n","        with tf.variable_scope(\"placeholder\"):\n","            self.input_x = tf.placeholder(tf.int32, (None, self.sequence_length))\n","            self.input_y = tf.placeholder(tf.int32, (None,))\n","            self.embedding_placeholder = tf.placeholder(tf.float32, (self.vocab_size, self.embedding_size))\n","\n","        # Embedding layer for input\n","        with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n","            W = tf.get_variable(\"W\", dtype=tf.float32,\n","                                initializer=tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n","                                trainable=self.trainable)\n","            self.embedding_init = W.assign(self.embedding_placeholder)\n","            embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n","            embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n","\n","        # Create a convolution + max_pool layer for each filter size\n","        pooled_outputs = []\n","        for filter_size in self.filter_sizes:\n","            with tf.variable_scope(\"conv-maxpool-%s\" % filter_size, reuse=tf.AUTO_REUSE):\n","                # Convolution Layer\n","                filter_shape = (filter_size, self.embedding_size, 1, self.num_filters)\n","                W = tf.get_variable(\"W\", dtype=tf.float32,\n","                                    initializer=tf.truncated_normal(filter_shape, stddev=0.1))\n","                b = tf.get_variable(\"b\", dtype=tf.float32,\n","                                    initializer=tf.constant(0.1, shape=(self.num_filters,)))\n","                conv = tf.nn.conv2d(\n","                    embedded_chars_expanded,\n","                    W,\n","                    strides=[1, 1, 1, 1],\n","                    padding=\"VALID\",\n","                    name=\"conv\")\n","                # Apply nonlinearity\n","                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n","                # Maxpooling over the outputs\n","                pooled = tf.nn.max_pool(\n","                    h,\n","                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\n","                    strides=[1, 1, 1, 1],\n","                    padding='VALID',\n","                    name=\"pool\")\n","                pooled_outputs.append(pooled)\n","\n","        # Combine all the pooled features\n","        num_filters_total = self.num_filters * len(self.filter_sizes)\n","        h_pool = tf.concat(pooled_outputs, 3)\n","        h_pool_flat = tf.reshape(h_pool, (-1, num_filters_total))\n","\n","        # Final (unnormalized) scores and predictions\n","        with tf.variable_scope(\"output\", reuse=tf.AUTO_REUSE):\n","            W = tf.get_variable(\n","                \"W\",\n","                shape=(num_filters_total, self.n_class),\n","                initializer=tf.contrib.layers.xavier_initializer())\n","            b = tf.Variable(tf.constant(0.1, shape=(self.n_class,)), name=\"b\")\n","            logits = tf.nn.xw_plus_b(h_pool_flat, W, b, name=\"logits\")\n","            self.prob = tf.reduce_max(tf.nn.softmax(logits), axis=1, name=\"prob\")\n","            self.prediction = tf.cast(tf.argmax(logits, 1), tf.int32, name=\"predictions\")\n","\n","        # Calculate mean cross-entropy loss\n","        with tf.variable_scope(\"loss\"):\n","            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=self.input_y)\n","            self.loss = tf.reduce_mean(losses)\n","\n","        with tf.variable_scope(\"train\", reuse=tf.AUTO_REUSE):\n","            global_step = tf.Variable(0, trainable=False)\n","            learning_rate = tf.train.exponential_decay(self.lr,\n","                                                       global_step,\n","                                                       1e+3,\n","                                                       0.9,\n","                                                       staircase=True)\n","            optimizer = tf.train.AdamOptimizer(learning_rate)\n","            self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n","\n","        # Accuracy\n","        with tf.variable_scope(\"accuracy\"):\n","            correct = tf.equal(self.prediction, self.input_y)\n","            self.accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","        self.sess.run(tf.global_variables_initializer())\n","\n","    def embedding_assign(self, embedding):\n","        return self.sess.run(self.embedding_init, feed_dict={self.embedding_placeholder: embedding})\n","\n","    def train(self, input_x, input_y):\n","        return self.sess.run([self.loss, self.train_op], feed_dict={self.input_x: input_x, self.input_y: input_y})\n","\n","    def predict(self, input_x):\n","        return self.sess.run((self.prediction, self.prob), feed_dict={self.input_x: input_x})\n","\n","    def get_accuracy(self, input_x, input_y):\n","        return self.sess.run(self.accuracy, feed_dict={self.input_x: input_x, self.input_y: input_y})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Hez74B3l1NiG","colab_type":"text"},"cell_type":"markdown","source":["## 4.Train Model"]},{"metadata":{"id":"0EHx9HOWwaKw","colab_type":"code","colab":{}},"cell_type":"code","source":["import os, json\n","import pandas as pd\n","\n","tf.reset_default_graph()\n","DIR = \"sentiment-model\"\n","\n","# build dataset\n","data = pd.read_csv('sentiment.txt', delimiter='\\t')\n","x_input = data.document\n","y_input = data.label\n","max_length = 30\n","print('데이터로부터 정보를 얻는 중입니다.')\n","embedding, vocab, vocab_size = make_embedding_vectors(list(x_input))\n","print('완료되었습니다.')\n","\n","# save vocab, vocab_size, max_length\n","with open('path.join(DIR, 'vocab.json')', 'w') as fp:\n","    json.dump(vocab, fp)\n","\n","# save configuration\n","with open('config.txt', 'w') as f:\n","    f.write(str(vocab_size) + '\\n')\n","    f.write(str(max_length))\n","\n","# open session\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","with tf.Session(config=config) as sess:\n","    # make model instance\n","    model = TextCNN(sess=sess, vocab_size=vocab_size, sequence_length=max_length, trainable=True)\n","\n","    # assign pretrained embedding vectors\n","    model.embedding_assign(embedding)\n","\n","    # make train batches\n","    x_input = sentence_to_index_morphs(x_input, vocab, max_length)\n","    batches = batch_iter(list(zip(x_input, y_input)), batch_size=64, num_epochs=10)\n","\n","    # model saver\n","    saver = tf.train.Saver(max_to_keep=1, keep_checkpoint_every_n_hours=0.5)\n","\n","    # train model\n","    print('모델 훈련을 시작합니다.')\n","    avgLoss = []\n","    for step, batch in enumerate(batches):\n","        x_train, y_train = zip(*batch)\n","        l, _ = model.train(x_train, y_train)\n","        avgLoss.append(l)\n","        if (step + 1) % 100 == 0:\n","            print('batch:', '%03d' % (step + 1), 'loss:', '%05f' % np.mean(avgLoss))\n","            saver.save(sess, os.path.join(DIR, \"model\"), global_step=step+1)\n","            avgLoss = []\n","    \n","    saver.save(sess, os.path.join(DIR, \"model\"), global_step=step+1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u_ziMIZ11Pfh","colab_type":"text"},"cell_type":"markdown","source":["## 5.Enjoy Sentiment Analysis!"]},{"metadata":{"id":"57tkq10Hws0g","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.reset_default_graph()\n","DIR = \"sentiment-model\"\n","\n","# load vocab, vocab_size, max_length\n","with open('path.join(DIR, 'vocab.json')', 'r') as fp:\n","    vocab = json.load(fp)\n","\n","# load configuration\n","with open('config.txt', 'r') as f:\n","    vocab_size = int(re.sub('\\n', '', f.readline()))\n","    max_length = int(f.readline())\n","\n","# open session\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","with tf.Session(config=config) as sess:\n","    # make model instance\n","    model = TextCNN(sess=sess, vocab_size=vocab_size, sequence_length=max_length, trainable=True)\n","\n","    # load trained model\n","    saver = tf.train.Saver()\n","    saver.restore(sess, tf.train.latest_checkpoint(DIR))\n","\n","    # inference\n","    while True:\n","        test = input(\"User >> \")\n","        if test == \"exit\":\n","            break\n","        speak = sentence_to_index_morphs([test], vocab, max_length)\n","        label, prob = model.predict(speak)\n","        if prob[0] < 0.6:\n","            response = '차분해 보이시네요 :)'\n","        else:\n","            if label[0] == 0:\n","                response = '기분이 좋지 않아 보여요 :('\n","            else:\n","                response = '기분이 좋아 보이시네요!'\n","        print(\"Bot >> \", response, \"\\n\")"],"execution_count":0,"outputs":[]}]}