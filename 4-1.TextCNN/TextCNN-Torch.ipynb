{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextCNN-Torch.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Cz8pUqzqyPIY","colab_type":"code","outputId":"2cdb76f5-5673-49c3-f3cc-040d95eee85a","executionInfo":{"status":"ok","timestamp":1550466282578,"user_tz":-540,"elapsed":15393,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","  reference : https://github.com/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN_Torch.ipynb\n","'''\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","\n","from torch import LongTensor as LT\n","from torch import FloatTensor as FT\n","\n","# Text-CNN Parameter\n","embedding_size = 2\n","sequence_length = 3\n","num_classes = 2  # 0 or 1\n","filter_sizes = [2, 2, 2] # n-gram window\n","num_filters = 3\n","\n","# words sentences\n","sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n","labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is bad.\n","\n","word_list = \" \".join(sentences).split()\n","word_list = list(set(word_list))\n","word_dict = {w: i for i, w in enumerate(word_list)}\n","vocab_size = len(word_dict)\n","\n","inputs = []\n","for sen in sentences:\n","    inputs.append(np.asarray([word_dict[n] for n in sen.split()]))\n","\n","targets = []\n","for out in labels:\n","    targets.append(out) # To using Torch Softmax Loss function\n","\n","input_batch = LT(inputs)\n","target_batch = LT(targets)\n","\n","\n","class TextCNN(nn.Module):\n","    def __init__(self, vocab_size, sequence_length, embedding_size, \n","                 filter_sizes, num_filters, num_classes):\n","        super(TextCNN, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.sequence_length = sequence_length\n","        self.embedding_size = embedding_size\n","        self.filter_sizes = filter_sizes\n","        self.num_filters = num_filters\n","        self.num_classes = num_classes\n","        \n","        self.num_filters_total = self.num_filters * len(self.filter_sizes)\n","        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n","        self.linear = nn.Linear(self.num_filters_total, self.num_classes)\n","        self.convs = nn.ModuleList([nn.Conv1d(self.embedding_size, self.num_filters, filter_size) for filter_size in self.filter_sizes])\n","        self.maxpools = nn.ModuleList([nn.MaxPool1d(self.sequence_length - filter_size + 1) for filter_size in self.filter_sizes])\n","        \n","    def forward(self, X):\n","        embedded_chars = self.embeddings(X) # [batch_size, sequence_length, embedding_size]\n","        embedded_chars = embedded_chars.permute(0, 2, 1) # [batch_size, embedding_size, sequence_length]\n","        \n","        pooled_outputs = []\n","        for i in range(len(self.filter_sizes)):\n","            conv = self.convs[i](embedded_chars)\n","            h = F.relu(conv)\n","            pooled = self.maxpools[i](h).permute(0, 2, 1)\n","            pooled_outputs.append(pooled)\n","        h_pool = torch.cat(pooled_outputs, len(self.filter_sizes) - 1) # [batch_size, 1, (num_filters * 3)]\n","        h_pool_flat = torch.reshape(h_pool, [-1, self.num_filters_total]) # [batch_size, 1 * (num_filters * 3)]\n","        logits = self.linear(h_pool_flat)\n","        return logits\n","\n","model = TextCNN(vocab_size, sequence_length, embedding_size, filter_sizes, num_filters, num_classes)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training\n","for epoch in range(5000):\n","    optimizer.zero_grad()\n","    output = model(input_batch)\n","\n","    # output : [batch_size, num_classes], target_batch : [batch_size] (LongTensor, not one-hot)\n","    loss = criterion(output, target_batch)\n","    if (epoch + 1) % 1000 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","    loss.backward()\n","    optimizer.step()\n","    \n","# Test\n","test_text = ['sorry hate you', 'you love me']\n","tests = []\n","for text in test_text:\n","    tests.append(np.asarray([word_dict[n] for n in text.split()]))\n","test_batch = LT(tests)\n","\n","# Predict\n","result = model(test_batch).data.max(1)[1]\n","for i, text in enumerate(test_text):\n","  if result[i] == 0:\n","      print(\"\\'\" + text + \"\\'\", \"is bad :(\")\n","  else:\n","      print(\"\\'\"+ text + \"\\'\", \"is good :)\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1000 cost = 0.003092\n","Epoch: 2000 cost = 0.000566\n","Epoch: 3000 cost = 0.000192\n","Epoch: 4000 cost = 0.000079\n","Epoch: 5000 cost = 0.000037\n","'sorry hate you' is bad :(\n","'you love me' is good :)\n"],"name":"stdout"}]}]}