{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SamHangSi-Tensor.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"KW_aTPXIeo5o","colab_type":"text"},"cell_type":"markdown","source":["## 1.Preprocess Tools"]},{"metadata":{"id":"DgpJgqcqahkv","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","  \n","'''\n","import numpy as np\n","import re, random\n","from collections import Counter\n","\n","def read_txt(data):\n","    lines = []\n","    with open(data, encoding='utf-16') as f:\n","        for line in f:\n","            lines.append(line)\n","    return lines\n","\n","def preprocess(data):\n","    lines = []\n","    for line in data:\n","        line = re.sub('<head>', '', line)\n","        line = re.sub('</head>', '', line)\n","        line = re.sub('<p>', '', line)\n","        line = re.sub('</p>', '', line)\n","        line = re.sub('\\n', '', line)\n","        line = re.sub('\\\"', '', line)\n","        lines += line.split('. ')\n","    return lines\n","\n","def build_vocab(data):\n","    word_counter = Counter()\n","    vocab = dict()\n","    reverse_vocab = dict()\n","    for line in data:\n","        word_counter.update(line.split())\n","    vocab['<PAD>'] = 0\n","    vocab['<UNK>'] = 1\n","    vocab_idx = 2\n","    for key, value in word_counter.most_common(len(word_counter)):\n","        vocab[key] = vocab_idx\n","        vocab_idx += 1\n","    for key, value in vocab.items():\n","        reverse_vocab[value] = key\n","\n","    vocab_size = len(vocab)\n","\n","    return vocab, reverse_vocab, vocab_size\n","\n","\n","def sentenceToIndex(lines, vocab):\n","    maxLength = 0\n","    inputSet = []\n","    targetSet = []\n","    data = []\n","\n","    if len(lines) == 1:\n","        line = lines[0]\n","        line = re.sub('<head>', '', line)\n","        line = re.sub('</head>', '', line)\n","        line = re.sub('<p>', '', line)\n","        line = re.sub('</p>', '', line)\n","        line = re.sub('\\n', '', line)\n","        line = re.sub('\\\"', '', line)\n","        line = re.sub('\\.', '', line)\n","        data = line.split(' ')\n","        indexes = []\n","        for word in data:\n","            if word in vocab.keys():\n","                indexes.append(vocab[word])\n","            else:\n","                indexes.append(vocab['<UNK>'])\n","        indexes.append(0)\n","        inputSet = [indexes[:-1]]\n","        targetSet = [indexes[1:]]\n","\n","    else:\n","        for line in lines:\n","            line = re.sub('<head>', '', line)\n","            line = re.sub('</head>', '', line)\n","            line = re.sub('<p>', '', line)\n","            line = re.sub('</p>', '', line)\n","            line = re.sub('\\n', '', line)\n","            line = re.sub('\\\"', '', line)\n","            line = re.sub('\\.', '', line)\n","            data.append(line.split(' '))\n","\n","        for line in data:\n","            if maxLength < len(line):\n","                maxLength = len(line)\n","\n","        for words in data:\n","            indexes = []\n","            for word in words:\n","                if word in vocab.keys():\n","                    indexes.append(vocab[word])\n","                else:\n","                    indexes.append(vocab['<UNK>'])\n","            for i in range(len(words), maxLength + 1):\n","                indexes.append(0)\n","            inputSet.append(indexes[:-1])\n","            targetSet.append(indexes[1:])\n","\n","    return inputSet, targetSet\n","\n","\n","def indexToSentence(lines, reverse_vocab):\n","    sentences = []\n","    if len(lines) == 1:\n","        line = lines[0]\n","        sentence = ''\n","        for index in line:\n","            if index == 0:\n","                sentence = sentence[:-1]\n","                break\n","            if index == 1:\n","                continue\n","            sentence += reverse_vocab[index] + ' '\n","        sentences.append(sentence)\n","\n","    else:\n","        for line in lines:\n","            sentence = ''\n","            for index in line:\n","                if index == 0:\n","                    sentence = sentence[:-1]\n","                    break\n","                if index == 1:\n","                    continue\n","                sentence += reverse_vocab[index] + ' '\n","            sentences.append(sentence)\n","    return sentences\n","\n","\n","def batch_iter(data, batch_size, num_epochs, shuffle=True):\n","    \"\"\"\n","    Generates a batch iterator for a dataset.\n","    \"\"\"\n","    data = np.array(data)\n","    data_size = len(data)\n","    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n","    for epoch in range(num_epochs):\n","        # Shuffle the data at each epoch\n","        if shuffle:\n","            shuffle_indices = np.random.permutation(np.arange(data_size))\n","            shuffled_data = data[shuffle_indices]\n","        else:\n","            shuffled_data = data\n","        for batch_num in range(num_batches_per_epoch):\n","            start_index = batch_num * batch_size\n","            end_index = min((batch_num + 1) * batch_size, data_size)\n","            yield shuffled_data[start_index:end_index]\n","\n","\n","def find_vocab(character, vocab):\n","    candidate = []\n","    if character == \"리\" or character == \"니\":\n","        character = \"이\"\n","    elif character == \"림\" or character == \"님\":\n","        character = \"임\"\n","    elif character == \"린\" or character == \"닌\":\n","        character = \"인\"\n","    elif character == \"랑\" or character == \"낭\":\n","        character = \"앙\"\n","    elif character == \"름\" or character == \"늠\":\n","        character = \"음\"\n","    elif character == \"랴\" or character == \"냐\":\n","        character = \"야\"\n","    elif character == \"력\" or character == \"녁\":\n","        character = \"역\"\n","    elif character == \"류\":\n","        character = \"유\"\n","    elif character == \"로\":\n","        character = \"노\"\n","    elif character == \"려\":\n","        character = \"여\"\n","    for key, value in vocab.items():\n","        if character == key[0]:\n","            candidate.append(value)\n","    try:\n","        result = random.sample(candidate, 1)[0]\n","    except ValueError:\n","        print(\"다른 글자를 입력해 주세요.\")\n","        return \"retry\"\n","    return result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uW5ynHUTeiuk","colab_type":"text"},"cell_type":"markdown","source":["## 2.Build Model"]},{"metadata":{"id":"OKjsmBgpaTQG","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","\n","class reRNN:\n","    def __init__(self, sess, vocab_size, lr=1e-1, max_step=50):\n","        self.sess = sess\n","        self.vocab_size = vocab_size\n","        self.lr = lr\n","        self.max_step = max_step\n","        self._build_net()\n","\n","    def _build_net(self):\n","        hidden_size = 128\n","        embedding_size = 300\n","        # placeholder for first_input, full_input, target_input\n","        with tf.variable_scope(\"placeholder\"):\n","            self.first_input = tf.placeholder(tf.int32, shape=(None,))\n","            self.full_input = tf.placeholder(tf.int32, shape=(None, None))\n","            input_length = tf.reduce_sum(tf.sign(self.full_input), axis=1)\n","            self.target_input = tf.placeholder(tf.int32, shape=(None, None))\n","            target_length = tf.reduce_sum(tf.sign(self.target_input), axis=1) + 1\n","\n","        # embedding for vocabs\n","        with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n","            self.embedding = tf.Variable(\n","                tf.random_uniform(shape=(self.vocab_size, embedding_size), minval=-1.0, maxval=1.0))\n","            embedded_full_input = tf.nn.embedding_lookup(self.embedding, self.full_input)\n","\n","        # recurrent operations\n","        with tf.variable_scope(\"recurrent\"):\n","            self.cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n","\n","            batch_size, max_time_step = tf.unstack(tf.shape(self.full_input))\n","\n","            outputs, _ = tf.nn.dynamic_rnn(self.cell,\n","                                           embedded_full_input,\n","                                           input_length,\n","                                           dtype=tf.float32)  # outputs: [batch, time, hidden], bw_outputs: [batch, time, hidden]\n","            outputs = tf.reshape(outputs, (-1, hidden_size))  # output: [batch*time, hidden]\n","\n","        # output with rnn memories\n","        with tf.variable_scope(\"output\", reuse=tf.AUTO_REUSE):\n","            self.W = tf.Variable(tf.truncated_normal(shape=(hidden_size, self.vocab_size)))\n","            self.b = tf.Variable(tf.constant(0.1, shape=(self.vocab_size,)))\n","            logits = tf.add(tf.matmul(outputs, self.W), self.b)  # logits: [batch*time, vocab_size]\n","            logits = tf.reshape(logits, (batch_size, max_time_step, -1))  # logits: [batch, time, vocab_size]\n","\n","        # loss calculation\n","        with tf.variable_scope(\"loss\"):\n","            self.loss = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(logits=logits,\n","                                                                        targets=self.target_input,\n","                                                                        weights=tf.sequence_mask(target_length,\n","                                                                                                 max_time_step,\n","                                                                                                 dtype=tf.float32)))\n","\n","        # train with clipped gradient\n","        with tf.variable_scope(\"train\", reuse=tf.AUTO_REUSE):\n","            global_step = tf.Variable(0, trainable=False)\n","            learning_rate = tf.train.exponential_decay(self.lr, global_step,\n","                                                       1e+3, 0.96, staircase=True)\n","            optimizer = tf.train.AdamOptimizer(learning_rate)\n","            gvs = optimizer.compute_gradients(self.loss)\n","            capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n","            self.train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n","\n","        # inference with first input (feed previous output to next input)\n","        with tf.variable_scope(\"inference\"):\n","            batch_size = tf.unstack(tf.shape(self.first_input))[0]\n","            state = self.cell.zero_state(batch_size, dtype=tf.float32)\n","            self.predictions = []\n","            prediction = 0\n","            for i in range(self.max_step):\n","                if i == 0:\n","                    input_ = tf.nn.embedding_lookup(self.embedding, self.first_input)\n","                else:\n","                    input_ = tf.nn.embedding_lookup(self.embedding, prediction)\n","                output, state = self.cell(input_, state)\n","                inf_logits = tf.add(tf.matmul(output, self.W), self.b)\n","                values, indices = tf.nn.top_k(inf_logits, 2)\n","                indices = tf.squeeze(indices, axis=0)\n","                index = tf.squeeze(tf.multinomial(values, 1), axis=0)\n","                prediction = tf.reshape(indices[index[0]], shape=(-1,))\n","                self.predictions.append(prediction)\n","            self.predictions = tf.stack(self.predictions, 1)\n","\n","        self.sess.run(tf.global_variables_initializer())\n","\n","    def setMaxStep(self, max_step):\n","        self.max_step = max_step\n","\n","    def train(self, full_input, target_input):\n","        return self.sess.run([self.loss, self.train_op],\n","                             feed_dict={self.full_input: full_input, self.target_input: target_input})\n","\n","    def inference(self, first_input):\n","        return self.sess.run(self.predictions, feed_dict={self.first_input: first_input})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ehLnCeBWeYaI","colab_type":"text"},"cell_type":"markdown","source":["## 3.Train Model"]},{"metadata":{"id":"HunNVRTKa1-f","colab_type":"code","outputId":"df826079-5ce3-47af-dc0e-a38d4be3d5c3","executionInfo":{"status":"ok","timestamp":1550558855179,"user_tz":-540,"elapsed":7149,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["import os, json\n","\n","tf.reset_default_graph()\n","DIR = \"samhangsi-model\"\n","\n","# read and build dataset\n","data = read_txt('novel.txt')\n","data = preprocess(data)\n","vocab, reverse_vocab, vocab_size = build_vocab(data)\n","\n","# save vocab\n","with open('vocab.json', 'w') as fp:\n","    json.dump(vocab, fp)\n","\n","# open session\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","\n","with tf.Session(config=config) as sess:\n","    # make model instance\n","    model = reRNN(sess=sess, vocab_size=vocab_size, lr=1e-1)\n","\n","    # make train batches\n","    batches = batch_iter(data, batch_size=64, num_epochs=100)\n","\n","    # model saver\n","    saver = tf.train.Saver(max_to_keep=1, keep_checkpoint_every_n_hours=0.5)\n","\n","    # train model\n","    print('모델 훈련을 시작합니다.')\n","    avgLoss = []\n","    for step, batch in enumerate(batches):\n","        x_train, y_train = sentenceToIndex(batch, vocab)\n","        l, _ = model.train(x_train, y_train)\n","        avgLoss.append(l)\n","        if (step + 1) % 100 == 0:\n","            print('batch:', '%04d' % (step + 1), 'loss:', '%.5f' % np.mean(avgLoss))\n","            saver.save(sess, os.path.join(DIR, 'my-model.ckpt'), global_step=(step+1))\n","            avgLoss = []"],"execution_count":0,"outputs":[{"output_type":"stream","text":["모델 훈련을 시작합니다.\n"],"name":"stdout"}]},{"metadata":{"id":"OkrIqNvbe1dD","colab_type":"text"},"cell_type":"markdown","source":["## 4.Enjoy SamHangSi"]},{"metadata":{"id":"xziUYIfqbF--","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.reset_default_graph()\n","DIR = \"samhangsi-model\"\n","\n","# load vocab, reverse_vocab, vocab_size\n","with open('vocab.json', 'r') as fp:\n","    vocab = json.load(fp)\n","reverse_vocab = dict()\n","for key, value in vocab.items():\n","    reverse_vocab[value] = key\n","vocab_size = len(vocab)\n","\n","# open session\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","with tf.Session(config=config) as sess:\n","    # make model instance\n","    model = reRNN(sess=sess, vocab_size=vocab_size, max_step=70)\n","\n","    # load trained model\n","    saver = tf.train.Saver()\n","    saver.restore(sess, tf.train.latest_checkpoint(DIR))\n","\n","    # inference\n","    while(True):\n","        chars = input('세 글자를 입력하세요: ')\n","        if chars == \"exit\":\n","            break\n","        if len(chars) != 3:\n","            print(\"세 글자를 입력해 주세요.\")\n","            continue\n","        for character in chars:\n","            number = find_vocab(character, vocab)\n","            if number == \"retry\":\n","                continue\n","            result = model.inference([number])\n","            print(reverse_vocab[number] + ' ' + indexToSentence(result, reverse_vocab)[0])\n","        print('')"],"execution_count":0,"outputs":[]}]}
