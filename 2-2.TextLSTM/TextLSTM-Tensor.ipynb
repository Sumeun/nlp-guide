{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextLSTM-Tensor.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"mGwiBqRHKveJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":377},"outputId":"17471e90-b166-4248-b34e-f731cdecc3f2","executionInfo":{"status":"ok","timestamp":1550474956897,"user_tz":-540,"elapsed":5676,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","'''\n","import tensorflow as tf\n","import numpy as np\n","\n","tf.reset_default_graph()\n","\n","char_arr = list('abcdefghijklmnopqrstuvwxyz')\n","word_dict = {n: i for i, n in enumerate(char_arr)}\n","number_dict = {i: w for i, w in enumerate(char_arr)}\n","n_class = len(word_dict) # number of class(=number of vocab)\n","\n","seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']\n","\n","# TextLSTM Parameters\n","n_step = 3\n","n_hidden = 128\n","\n","def make_batch(seq_data):\n","    input_batch, target_batch = [], []\n","\n","    for seq in seq_data:\n","        input = [word_dict[n] for n in seq[:-1]] # 'm', 'a' , 'k' is input\n","        target = word_dict[seq[-1]] # 'e' is target\n","        input_batch.append(np.eye(n_class)[input])\n","        target_batch.append(np.eye(n_class)[target])\n","\n","    return input_batch, target_batch\n","\n","# Model\n","class TextLSTM(object):\n","    def __init__(self, sess, n_step, n_hidden, n_class):\n","        self.sess = sess\n","        self.n_step = n_step\n","        self.n_hidden = n_hidden\n","        self.n_class = n_class\n","        self._build_model()\n","    \n","    def _build_model(self):\n","        self.X = tf.placeholder(tf.float32, [None, self.n_step, self.n_class]) # [batch_size, n_step, n_class]\n","        self.Y = tf.placeholder(tf.float32, [None, self.n_class])         # [batch_size, n_class]\n","        \n","        W = tf.Variable(tf.random_normal([self.n_hidden, self.n_class]))\n","        b = tf.Variable(tf.random_normal([self.n_class]))\n","        \n","        cell = tf.nn.rnn_cell.BasicLSTMCell(self.n_hidden)\n","        outputs, states = tf.nn.dynamic_rnn(cell, self.X, dtype=tf.float32)\n","        outputs = tf.transpose(outputs, [1, 0, 2]) # [n_step, batch_size, n_hidden]\n","        outputs = outputs[-1]  # [batch_size, n_hidden]\n","        self.logits = tf.matmul(states[-1], W) + b # model : [batch_size, n_class]\n","        \n","        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.Y))\n","        self.optimizer = tf.train.AdamOptimizer(0.001).minimize(self.cost)\n","        self.prediction = tf.cast(tf.argmax(self.logits, 1), tf.int32)\n","\n","        tf.global_variables_initializer().run()\n","    \n","    def train(self, inputs, labels):\n","        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: inputs, self.Y: labels})\n","     \n","    def predict(self, inputs):\n","        return self.sess.run([self.prediction], feed_dict={self.X: inputs})\n","        \n","\n","# Training\n","run_config = tf.ConfigProto()\n","run_config.gpu_options.allow_growth=True\n","with tf.Session(config=run_config) as sess:\n","    input_batch, target_batch = make_batch(seq_data)\n","    model = TextLSTM(sess, n_step, n_hidden, n_class)\n","    for epoch in range(1000):\n","        loss, _ = model.train(input_batch, target_batch)\n","        if (epoch + 1) % 100 == 0:\n","            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","    predict =  model.predict(input_batch)[0]\n","\n","inputs = [sen[:3] for sen in seq_data]\n","for i, input in enumerate(inputs):\n","    print(input, '->', number_dict[predict[i]])"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Epoch: 0100 cost = 0.019288\n","Epoch: 0200 cost = 0.004340\n","Epoch: 0300 cost = 0.001885\n","Epoch: 0400 cost = 0.001052\n","Epoch: 0500 cost = 0.000671\n","Epoch: 0600 cost = 0.000466\n","Epoch: 0700 cost = 0.000342\n","Epoch: 0800 cost = 0.000262\n","Epoch: 0900 cost = 0.000206\n","Epoch: 1000 cost = 0.000167\n","mak -> e\n","nee -> d\n","coa -> l\n","wor -> d\n","lov -> e\n","hat -> e\n","liv -> e\n","hom -> e\n","has -> h\n","sta -> r\n"],"name":"stdout"}]}]}