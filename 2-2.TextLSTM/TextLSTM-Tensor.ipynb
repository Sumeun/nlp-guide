{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextLSTM-Tensor.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"mGwiBqRHKveJ","colab_type":"code","outputId":"67fe0514-61e2-47d3-a90b-7cc13af053c3","executionInfo":{"status":"ok","timestamp":1550638427008,"user_tz":-540,"elapsed":6125,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","  \n","'''\n","import tensorflow as tf\n","import numpy as np\n","\n","tf.reset_default_graph()\n","\n","char_arr = list('abcdefghijklmnopqrstuvwxyz')\n","word_dict = {n: i for i, n in enumerate(char_arr)}\n","number_dict = {i: w for i, w in enumerate(char_arr)}\n","\n","seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']\n","\n","# TextLSTM Parameters\n","vocab_size = len(word_dict)\n","n_embed = 20\n","n_class = len(word_dict)\n","n_step = 3\n","n_hidden = 128\n","\n","def make_batch(seq_data):\n","    input_batch, target_batch = [], []\n","    for seq in seq_data:\n","        input_batch.append([word_dict[n] for n in seq[:-1]])\n","        target_batch.append([word_dict[seq[-1]]])\n","        \n","    return input_batch, target_batch\n","\n","# Model\n","class TextLSTM(object):\n","    def __init__(self, sess, vocab_size, n_embed, n_step, n_hidden, n_class):\n","        self.sess = sess\n","        self.vocab_size = vocab_size\n","        self.n_embed = n_embed\n","        self.n_step = n_step\n","        self.n_hidden = n_hidden\n","        self.n_class = n_class\n","        self._build_model()\n","    \n","    def _build_model(self):\n","        with tf.variable_scope(\"placeholder\"):\n","            self.X = tf.placeholder(tf.int32, [None, self.n_step]) # [batch_size, n_step]\n","            self.Y = tf.placeholder(tf.int32, [None, 1])         # [batch_size, 1]\n","        \n","        with tf.variable_scope(\"embedding\"):\n","            W = tf.Variable(tf.random_normal([self.vocab_size, self.n_embed]))\n","            b = tf.Variable(tf.random_normal([self.n_embed]))\n","            embedded = tf.nn.embedding_lookup(W, self.X)\n","        \n","        with tf.variable_scope(\"rnn\"):\n","            cell = tf.nn.rnn_cell.BasicLSTMCell(self.n_hidden)\n","            outputs, states = tf.nn.dynamic_rnn(cell, embedded, dtype=tf.float32)\n","            outputs = tf.transpose(outputs, [1, 0, 2]) # [n_step, batch_size, n_hidden]\n","            outputs = outputs[-1] # [batch_size, n_hidden]\n","        \n","        with tf.variable_scope(\"output\"):\n","            W = tf.Variable(tf.random_normal([self.n_hidden, self.n_class]))\n","            b = tf.Variable(tf.random_normal([self.n_class]))\n","            logits = tf.matmul(outputs, W) + b\n","        \n","        logits = tf.expand_dims(logits, 1)\n","        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=self.Y))\n","        self.optimizer = tf.train.AdamOptimizer(0.001).minimize(self.cost)\n","        self.prediction = tf.cast(tf.argmax(logits, -1), tf.int32)\n","\n","        tf.global_variables_initializer().run()\n","    \n","    def train(self, inputs, labels):\n","        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: inputs, self.Y: labels})\n","     \n","    def predict(self, inputs):\n","        return self.sess.run(self.prediction, feed_dict={self.X: inputs})\n","        \n","\n","# Training\n","run_config = tf.ConfigProto()\n","run_config.gpu_options.allow_growth=True\n","with tf.Session(config=run_config) as sess:\n","    input_batch, target_batch = make_batch(seq_data)\n","    model = TextLSTM(sess, vocab_size, n_embed, n_step, n_hidden, n_class)\n","    for epoch in range(1000):\n","        loss, _ = model.train(input_batch, target_batch)\n","        if (epoch + 1) % 100 == 0:\n","            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","    predict =  model.predict(input_batch)\n","\n","inputs = [sen[:3] for sen in seq_data]\n","for i, input in enumerate(inputs):\n","    print(input, '->', number_dict[predict[i][0]])"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Epoch: 0100 cost = 0.001793\n","Epoch: 0200 cost = 0.000833\n","Epoch: 0300 cost = 0.000471\n","Epoch: 0400 cost = 0.000300\n","Epoch: 0500 cost = 0.000207\n","Epoch: 0600 cost = 0.000151\n","Epoch: 0700 cost = 0.000115\n","Epoch: 0800 cost = 0.000090\n","Epoch: 0900 cost = 0.000072\n","Epoch: 1000 cost = 0.000059\n","mak -> e\n","nee -> d\n","coa -> l\n","wor -> d\n","lov -> e\n","hat -> e\n","liv -> e\n","hom -> e\n","has -> h\n","sta -> r\n"],"name":"stdout"}]}]}