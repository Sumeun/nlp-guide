{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextLSTM-Torch.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"fZZu7_ViP0Ab","colab_type":"code","outputId":"6acd969c-cf82-4935-be3b-188c1ba1cbe1","executionInfo":{"status":"ok","timestamp":1550471023704,"user_tz":-540,"elapsed":3462,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","'''\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","char_arr = list('abcdefghijklmnopqrstuvwxyz')\n","word_dict = {n: i for i, n in enumerate(char_arr)}\n","number_dict = {i: w for i, w in enumerate(char_arr)}\n","n_class = len(word_dict) # number of class(=number of vocab)\n","\n","seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']\n","\n","# TextLSTM Parameters\n","n_step = 3\n","n_hidden = 128\n","\n","def make_batch(seq_data):\n","    input_batch, target_batch = [], []\n","\n","    for seq in seq_data:\n","        input = [word_dict[n] for n in seq[:-1]] # 'm', 'a' , 'k' is input\n","        target = word_dict[seq[-1]] # 'e' is target\n","        input_batch.append(np.eye(n_class)[input])\n","        target_batch.append(target)\n","\n","    return torch.Tensor(input_batch), torch.LongTensor(target_batch)\n","\n","class TextLSTM(nn.Module):\n","    def __init__(self, n_hidden, n_class):\n","        super(TextLSTM, self).__init__()\n","        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden)\n","        self.linear = nn.Linear(n_hidden, n_class)\n","\n","    def forward(self, X):\n","        input = X.transpose(0, 1)  # X : [n_step, batch_size, n_class]\n","\n","        hidden_state = torch.zeros(1, len(X), n_hidden)   # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n","        cell_state = torch.zeros(1, len(X), n_hidden)     # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n","\n","        outputs, (hidden, _) = self.lstm(input, (hidden_state, cell_state))\n","        hidden = hidden.squeeze(0)\n","        model = self.linear(hidden)\n","        return model\n","\n","input_batch, target_batch = make_batch(seq_data)\n","\n","model = TextLSTM(n_hidden, n_class)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","output = model(input_batch)\n","\n","# Training\n","for epoch in range(1000):\n","    optimizer.zero_grad()\n","\n","    output = model(input_batch)\n","    loss = criterion(output, target_batch)\n","    if (epoch + 1) % 100 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","inputs = [sen[:3] for sen in seq_data]\n","\n","predict = model(input_batch).data.max(1, keepdim=True)[1].squeeze().numpy()\n","for i, input in enumerate(inputs):\n","    print(input, '->', number_dict[predict[i]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 0100 cost = 0.545306\n","Epoch: 0200 cost = 0.035130\n","Epoch: 0300 cost = 0.009809\n","Epoch: 0400 cost = 0.004519\n","Epoch: 0500 cost = 0.002628\n","Epoch: 0600 cost = 0.001731\n","Epoch: 0700 cost = 0.001232\n","Epoch: 0800 cost = 0.000924\n","Epoch: 0900 cost = 0.000720\n","Epoch: 1000 cost = 0.000577\n","mak -> e\n","nee -> d\n","coa -> l\n","wor -> d\n","lov -> e\n","hat -> e\n","liv -> e\n","hom -> e\n","has -> h\n","sta -> r\n"],"name":"stdout"}]}]}