{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq-Torch.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"pKP4H2noba7i","colab_type":"code","outputId":"b2218be1-b5f7-43a4-8bc6-ab142113df0e","executionInfo":{"status":"ok","timestamp":1550645955943,"user_tz":-540,"elapsed":9023,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}},"colab":{"base_uri":"https://localhost:8080/","height":305}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","  reference : https://github.com/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq_Torch.ipynb\n","  \n","'''\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","from torch import LongTensor as LT\n","\n","# S: Symbol that shows starting of decoding input\n","# E: Symbol that shows starting of decoding output\n","# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n","\n","char_arr = list('SEPabcdefghijklmnopqrstuvwxyz')\n","num_dic = {n: i for i, n in enumerate(char_arr)}\n","\n","seq_data = [['man', 'woman'], ['men', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n","\n","# Seq2Seq Parameter\n","vocab_size = len(num_dic)\n","n_embed = 5\n","n_step = 5\n","n_hidden = 128\n","n_class = len(num_dic) # number of class(=number of vocab)\n","\n","def make_batch(seq_data):\n","    input_batch, output_batch, target_batch = [], [], []\n","\n","    for seq in seq_data:\n","        for i in range(2):\n","            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n","\n","        input = [num_dic[n] for n in seq[0]]\n","        output = [num_dic[n] for n in ('S' + seq[1])]\n","        target = [num_dic[n] for n in (seq[1] + 'E')]\n","\n","        input_batch.append(input)\n","        output_batch.append(output)\n","        target_batch.append(target)\n","\n","    return LT(input_batch), LT(output_batch), LT(target_batch)\n","\n","\n","# Model\n","class Seq2Seq(nn.Module):\n","    def __init__(self, vocab_size, n_embed, n_hidden, n_class):\n","        super(Seq2Seq, self).__init__()\n","\n","        self.enc_lstm = nn.LSTM(input_size=n_embed, hidden_size=n_hidden)\n","        self.dec_lstm = nn.LSTM(input_size=n_embed, hidden_size=n_hidden)\n","        self.embedding = nn.Embedding(vocab_size, n_embed)\n","        self.linear = nn.Linear(n_hidden, n_class)\n","\n","    def forward(self, enc_input, dec_input):\n","        enc_input = self.embedding(enc_input).transpose(0, 1) # enc_input: [n_step, batch_size, n_embed]\n","        dec_input = self.embedding(dec_input).transpose(0, 1) # dec_input: [n_step+1, batch_size, n_embed]\n","\n","        _, enc_states = self.enc_lstm(enc_input)\n","        \n","        outputs, _ = self.dec_lstm(dec_input, enc_states) # outputs: [n_step+1, batch_size, n_hidden]\n","        outputs = outputs.transpose(0, 1) # outputs: [batch_size, n_step+1, n_hidden]\n","        model = self.linear(outputs).transpose(1, 2) # mode: [batch_size, n_class, n_step+1]\n","        return model\n","\n","\n","input_batch, output_batch, target_batch = make_batch(seq_data)\n","\n","model = Seq2Seq(vocab_size, n_embed, n_hidden, n_class)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(1000):\n","    optimizer.zero_grad()\n","    \n","    output = model(input_batch, output_batch)\n","    loss = criterion(output, target_batch)\n","    if (epoch + 1) % 100 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","        \n","    loss.backward()\n","    optimizer.step()\n","\n","\n","# Test\n","def translate(word):\n","    seq_data = [word, 'P' * len(word)]\n","    \n","    input_batch, output_batch, _ = make_batch([seq_data])\n","\n","    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n","    output = model(input_batch, output_batch)\n","    # output : [max_len+1(=6), batch_size(=1), n_class]\n","    predict = output.data.max(1, keepdim=True)[1] # select n_class dimension\n","    decoded = [char_arr[i] for i in predict.squeeze()]\n","    if 'E' in decoded:\n","        end = decoded.index('E')\n","        translated = ''.join(decoded[:end])\n","    else:\n","        translated = ''.join(decoded)\n","    return translated.replace('P', '')\n","\n","print('test')\n","print('man ->', translate('man'))\n","print('men ->', translate('men'))\n","print('king ->', translate('king'))\n","print('black ->', translate('black'))\n","print('upp ->', translate('upp'))"],"execution_count":90,"outputs":[{"output_type":"stream","text":["Epoch: 0100 cost = 0.370827\n","Epoch: 0200 cost = 0.033180\n","Epoch: 0300 cost = 0.012336\n","Epoch: 0400 cost = 0.006958\n","Epoch: 0500 cost = 0.004573\n","Epoch: 0600 cost = 0.003269\n","Epoch: 0700 cost = 0.002465\n","Epoch: 0800 cost = 0.001925\n","Epoch: 0900 cost = 0.001549\n","Epoch: 1000 cost = 0.001274\n","test\n","man -> woman\n","men -> women\n","king -> queen\n","black -> white\n","upp -> down\n"],"name":"stdout"}]}]}