{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Conversation-Tensor.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"mq6vWOFri7lo","colab_type":"text"},"cell_type":"markdown","source":["## 1.Preprocess Tools"]},{"metadata":{"id":"PtMyHg0TfuSg","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","  \n","'''\n","import re\n","from collections import Counter\n","import numpy as np\n","\n","def read_txt(data):\n","    lines = []\n","    with open(data, encoding='utf-8') as f:\n","        for line in f:\n","            lines.append(re.sub('\\n', '', line))\n","    return lines\n","\n","def tokenizer(sentence):\n","    tokens = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence)\n","    return tokens\n","\n","\n","def build_character(sentences):\n","    word_counter = Counter()\n","    vocab = dict()\n","    reverse_vocab = dict()\n","\n","    for sentence in sentences:\n","        tokens = list(sentence)\n","        word_counter.update(tokens)\n","\n","    vocab['<PAD>'] = 0\n","    vocab['<GO>'] = 1\n","    vocab['<UNK>'] = 2\n","    vocab_idx = 3\n","\n","    for key, value in word_counter.most_common(len(word_counter)):\n","        vocab[key] = vocab_idx\n","        vocab_idx += 1\n","\n","    for key, value in vocab.items():\n","        reverse_vocab[value] = key\n","\n","    vocab_size = len(vocab.keys())\n","\n","    return vocab, reverse_vocab, vocab_size\n","\n","\n","def build_vocab(sentences):\n","    word_counter = Counter()\n","    vocab = dict()\n","    reverse_vocab = dict()\n","\n","    for sentence in sentences:\n","        tokens = tokenizer(sentence)\n","        word_counter.update(tokens)\n","\n","    vocab['<PAD>'] = 0\n","    vocab['<GO>'] = 1\n","    vocab['<UNK>'] = 2\n","    vocab_idx = 3\n","\n","    for key, value in word_counter.most_common(len(word_counter)):\n","        vocab[key] = vocab_idx\n","        vocab_idx += 1\n","\n","    for key, value in vocab.items():\n","        reverse_vocab[value] = key\n","\n","    vocab_size = len(vocab.keys())\n","\n","    return vocab, reverse_vocab, vocab_size\n","\n","\n","def sentence_to_char_index(lines, vocab, is_target=False):\n","    tokens = []\n","    indexes = []\n","    max_len = 0\n","\n","    if len(lines) == 1:\n","        tokens = list(lines[0])\n","        for token in tokens:\n","            if token in vocab.keys():\n","                indexes.append(vocab[token])\n","            else:\n","                indexes.append(vocab['<UNK>'])\n","\n","    else:\n","        for sentence in lines:\n","            token = list(sentence)\n","            tokens.append(token)\n","            length = len(token)\n","            if max_len < length:\n","                if is_target == True:\n","                    max_len = length + 1\n","                else:\n","                    max_len = length\n","\n","        for token in tokens:\n","            temp = token\n","            for _ in range(len(temp), max_len):\n","                temp.append('<PAD>')\n","            index = []\n","            for char in temp:\n","                if char in vocab.keys():\n","                    index.append(vocab[char])\n","                else:\n","                    index.append(vocab['<UNK>'])\n","            indexes.append(index)\n","\n","    return indexes\n","\n","\n","def sentence_to_word_index(lines, vocab, is_target=False):\n","    tokens = []\n","    indexes = []\n","    max_len = 0\n","\n","    if type(lines) is str:\n","        tokens = tokenizer(lines)\n","        for token in tokens:\n","            if token in vocab.keys():\n","                indexes.append(vocab[token])\n","            else:\n","                indexes.append(vocab['<UNK>'])\n","\n","    else:\n","        for sentence in lines:\n","            token = tokenizer(sentence)\n","            tokens.append(token)\n","            length = len(token)\n","            if max_len < length:\n","                if is_target == True:\n","                    max_len = length + 1\n","                else:\n","                    max_len = length\n","\n","        for token in tokens:\n","            temp = token\n","            for _ in range(len(temp), max_len):\n","                temp.append('<PAD>')\n","            index = []\n","            for char in temp:\n","                if char in vocab.keys():\n","                    index.append(vocab[char])\n","                else:\n","                    index.append(vocab['<UNK>'])\n","            indexes.append(index)\n","\n","    return indexes\n","\n","\n","def make_dataset(data):\n","    input = []\n","    target = []\n","    for i in range(len(data)-1):\n","        input.append(data[i])\n","        target.append(data[i+1])\n","    return input, target\n","\n","\t\n","def make_dataset_for_translation(data):\n","    input = []\n","    target = []\n","    for i in range(0, len(data), 2):\n","        input.append(data[i])\n","        target.append(data[i+1])\n","    return input, target\n","\n","\n","def batch_iter(data, batch_size, num_epochs, shuffle=True):\n","    \"\"\"\n","    Generates a batch iterator for a dataset.\n","    \"\"\"\n","    data = np.array(data)\n","    data_size = len(data)\n","    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n","    for epoch in range(num_epochs):\n","        # Shuffle the data at each epoch\n","        if shuffle:\n","            shuffle_indices = np.random.permutation(np.arange(data_size))\n","            shuffled_data = data[shuffle_indices]\n","        else:\n","            shuffled_data = data\n","        for batch_num in range(num_batches_per_epoch):\n","            start_index = batch_num * batch_size\n","            end_index = min((batch_num + 1) * batch_size, data_size)\n","            yield shuffled_data[start_index:end_index]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ra0VAbWOi_id","colab_type":"text"},"cell_type":"markdown","source":["## 2.Build Model"]},{"metadata":{"id":"lqt0Sh1df0Zv","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","\n","class seq2seq:\n","\n","    def __init__(self, sess, encoder_vocab_size, decoder_vocab_size, lr=1e-1, max_step=50,\n","                 embedding_size=300, encoder_hidden_size=128):\n","        self.sess = sess\n","        self.encoder_vocab_size = encoder_vocab_size\n","        self.decoder_vocab_size = decoder_vocab_size\n","        self.lr = lr\n","        self.max_step = max_step\n","        self.embedding_size = embedding_size\n","        self.encoder_hidden_size = encoder_hidden_size\n","        self.decoder_hidden_size = encoder_hidden_size * 2\n","        self._build_net()\n","\n","    def _build_net(self):\n","        # placeholder for encoder_input, decoder_input, decoder_target\n","        with tf.variable_scope(\"placeholder\"):\n","            self.encoder_inputs = tf.placeholder(dtype=tf.int32, shape=(None, None), name='encoder_inputs')\n","            encoder_inputs_length = tf.reduce_sum(tf.sign(self.encoder_inputs), axis=1)\n","\n","            self.decoder_targets = tf.placeholder(dtype=tf.int32, shape=(None, None), name='decoder_inputs')\n","            decoder_targets_length = tf.reduce_sum(tf.sign(self.decoder_targets), axis=1) + 1\n","            batch_size, decoder_max_length = tf.unstack(tf.shape(self.decoder_targets))\n","            decoder_inputs = tf.concat((tf.transpose([tf.ones([batch_size], dtype=tf.int32)], perm=(1,0)),\n","                                        self.decoder_targets[:,:-1]), axis=1)\n","\n","        # embedding for encoder, decoder inputs\n","        with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n","            embedding = tf.get_variable('embedding',\n","                                        dtype=tf.float32,\n","                                        initializer=tf.random_uniform((self.encoder_vocab_size,\n","                                                                       self.embedding_size),\n","                                                                       minval=-1.0, maxval=1.0))\n","            embedded_encoder_inputs = tf.nn.embedding_lookup(embedding, self.encoder_inputs)\n","            embedded_decoder_inputs = tf.nn.embedding_lookup(embedding, decoder_inputs)\n","\n","        # encoder operations\n","        with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n","            self.encoder_fw_cell = tf.nn.rnn_cell.LSTMCell(self.encoder_hidden_size)\n","            self.encoder_bw_cell = tf.nn.rnn_cell.LSTMCell(self.encoder_hidden_size)\n","\n","            ((_, _),\n","             (encoder_fw_last_state, encoder_bw_last_state)) = tf.nn.bidirectional_dynamic_rnn(self.encoder_fw_cell,\n","                                                                                               self.encoder_bw_cell,\n","                                                                                               embedded_encoder_inputs,\n","                                                                                               encoder_inputs_length,\n","                                                                                               dtype=tf.float32)\n","\n","            encoder_final_state_c = tf.concat((encoder_fw_last_state.c, encoder_bw_last_state.c), 1)\n","            encoder_final_state_h = tf.concat((encoder_fw_last_state.h, encoder_bw_last_state.h), 1)\n","            self.encoder_final_state = tf.nn.rnn_cell.LSTMStateTuple(encoder_final_state_c, encoder_final_state_h)\n","\n","        # decoder operations with last encoder hidden state as an initial hidden state\n","        with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n","            self.decoder_cell = tf.nn.rnn_cell.LSTMCell(self.decoder_hidden_size)\n","            decoder_output, decoder_last_state = tf.nn.dynamic_rnn(self.decoder_cell,\n","                                                                   embedded_decoder_inputs,\n","                                                                   initial_state=self.encoder_final_state)\n","\n","        # output with decoder memories\n","        with tf.variable_scope(\"output\"):\n","            self.W = tf.get_variable('W', initializer=tf.truncated_normal(shape=(self.decoder_hidden_size, self.decoder_vocab_size)))\n","            self.b = tf.get_variable('b', initializer=tf.constant(0.1, shape=(self.decoder_vocab_size,)))\n","\n","            batch_size, max_time_step = tf.unstack(tf.shape(self.decoder_targets))\n","            decoder_output = tf.reshape(decoder_output, [-1, self.decoder_hidden_size]) # [batch_size*time_step, decoder_hidden_size]\n","            logits = tf.add(tf.matmul(decoder_output, self.W), self.b) #\n","            logits = tf.reshape(logits, [batch_size, max_time_step, -1])\n","\n","        # loss calculation\n","        with tf.variable_scope(\"loss\"):\n","            self.loss = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(logits=logits,\n","                                                                        targets=self.decoder_targets,\n","                                                                        weights=tf.sequence_mask(decoder_targets_length,\n","                                                                                                 decoder_max_length,\n","                                                                                                 dtype=tf.float32)))\n","\n","        # train with clipped gradient\n","        with tf.variable_scope(\"train\", reuse=tf.AUTO_REUSE):\n","            global_step = tf.Variable(0, trainable=False)\n","            learning_rate = tf.train.exponential_decay(self.lr, global_step,\n","                                                       1e+3, 0.96, staircase=True)\n","            optimizer = tf.train.AdamOptimizer(learning_rate)\n","            gvs = optimizer.compute_gradients(self.loss)\n","            capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n","            self.train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n","\n","        # inference with user's input (feed previous output to next input)\n","        with tf.variable_scope(\"inference\"):\n","            batch_size = tf.unstack(tf.shape(self.encoder_inputs))[0]\n","            go_time_slice = tf.ones([batch_size], dtype=tf.int32, name='GO')\n","            self.predictions = []\n","            prediction = None\n","            state = self.encoder_final_state\n","            for i in range(self.max_step):\n","                if i == 0:\n","                    input_ = tf.nn.embedding_lookup(embedding, go_time_slice)\n","                else:\n","                    input_ = tf.nn.embedding_lookup(embedding, prediction)\n","\n","                output, state = self.decoder_cell(input_, state)\n","                logits = tf.add(tf.matmul(output, self.W), self.b)\n","                prediction = tf.argmax(logits, 1)\n","                self.predictions.append(prediction)\n","            self.predictions = tf.stack(self.predictions, 1)\n","        \n","        self.sess.run(tf.global_variables_initializer())\n","\n","    def train(self, encoder_inputs, decoder_targets):\n","        return self.sess.run([self.loss, self.train_op], feed_dict={self.encoder_inputs:encoder_inputs,\n","                                                                    self.decoder_targets:decoder_targets})\n","\n","    def inference(self, encoder_inputs):\n","        return self.sess.run(self.predictions, feed_dict={self.encoder_inputs: encoder_inputs})\n","\n","    def setMaxStep(self, max_step):\n","        self.max_step = max_step"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2QwPs_IWjFYc","colab_type":"text"},"cell_type":"markdown","source":["## 3.Train Model"]},{"metadata":{"id":"m42x6v2Ef39O","colab_type":"code","outputId":"af243899-c5a1-4471-d7a1-30f9417dc040","executionInfo":{"status":"ok","timestamp":1550559123582,"user_tz":-540,"elapsed":55867,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"cell_type":"code","source":["import os, re, json\n","\n","tf.reset_default_graph()\n","DIR = \"conversation-model\"\n","\n","# read and build dataset\n","data = read_txt('dialog.txt')\n","vocab, reverse_vocab, vocab_size = build_character(data)\n","\n","# save vocab\n","with open('vocab.json', 'w') as fp:\n","    json.dump(vocab, fp)\n","\n","# open session\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","with tf.Session(config=config) as sess:\n","  \n","    # make model instance\n","    model = seq2seq(sess, encoder_vocab_size=vocab_size, decoder_vocab_size=vocab_size)\n","\n","    # make train batches\n","    inputs, target = make_dataset(data)\n","    batches = batch_iter(list(zip(inputs, target)), batch_size=64, num_epochs=500)\n","\n","    # model saver\n","    saver = tf.train.Saver(max_to_keep=1, keep_checkpoint_every_n_hours=0.5)\n","\n","    # train model\n","    print('모델 훈련을 시작합니다.')\n","    avgLoss = []\n","    for step, batch in enumerate(batches):\n","        x_train, y_train = zip(*batch)\n","        x_train = sentence_to_char_index(x_train, vocab, is_target=False)\n","        y_train = sentence_to_char_index(y_train, vocab, is_target=True)\n","        l, _ = model.train(x_train, y_train)\n","        avgLoss.append(l)\n","        if (step + 1) % 100 == 0:\n","            print('batch:', '%04d' % (step + 1), 'loss:', '%.5f' % np.mean(avgLoss))\n","            saver.save(sess, os.path.join(DIR, \"model\"), global_step=step+1)\n","            avgLoss = []"],"execution_count":0,"outputs":[{"output_type":"stream","text":["모델 훈련을 시작합니다.\n","batch: 0100 loss: 0.65784\n","batch: 0200 loss: 0.00492\n","batch: 0300 loss: 0.00356\n","batch: 0400 loss: 0.00325\n","batch: 0500 loss: 0.00310\n"],"name":"stdout"}]},{"metadata":{"id":"blo0IPo2jG_B","colab_type":"text"},"cell_type":"markdown","source":["## 4.Enjoy Conversation"]},{"metadata":{"id":"yvondpw9gy2P","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.reset_default_graph()\n","DIR = \"conversation-model\"\n","\n","# load vocab, reverse_vocab, vocab_size\n","with open('vocab.json', 'r') as fp:\n","    vocab = json.load(fp)\n","reverse_vocab = dict()\n","for key, value in vocab.items():\n","    reverse_vocab[value] = key\n","vocab_size = len(vocab)\n","\n","# open session\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","\n","with tf.Session(config=config) as sess:\n","    # make model instance\n","    model = seq2seq(sess, encoder_vocab_size=vocab_size, decoder_vocab_size=vocab_size, max_step=50)\n","\n","    # load trained model\n","    saver = tf.train.Saver()\n","    saver.restore(sess, tf.train.latest_checkpoint(DIR))\n","\n","    # inference\n","    while True:\n","        test = input('User >> ')\n","        if test == \"exit\":\n","            break\n","        speak = sentence_to_char_index([test], vocab, False)\n","        result = model.inference([speak])\n","        for sentence in result:\n","            response = ''\n","            for index in sentence:\n","                if index == 0:\n","                    break\n","                response += reverse_vocab[index]\n","            print(\"Bot >> \", response, \"\\n\")\n"],"execution_count":0,"outputs":[]}]}
