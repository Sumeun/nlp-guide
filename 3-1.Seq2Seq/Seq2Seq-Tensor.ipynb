{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq-Tensor.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"WqJTCkyRUVNo","colab_type":"code","outputId":"0a86bb1a-2784-41dd-c5d1-ee5637581beb","executionInfo":{"status":"ok","timestamp":1550646467718,"user_tz":-540,"elapsed":18626,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}},"colab":{"base_uri":"https://localhost:8080/","height":305}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","  reference : https://github.com/golbin/TensorFlow-Tutorials/blob/master/10%20-%20RNN/03%20-%20Seq2Seq.py\n","              https://github.com/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq_Tensor.ipynb\n","              \n","'''\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, Dense\n","import numpy as np\n","\n","tf.reset_default_graph()\n","# S: Symbol that shows starting of decoding input\n","# E: Symbol that shows starting of decoding output\n","# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n","\n","char_arr = list('SEPabcdefghijklmnopqrstuvwxyz')\n","num_dic = {n: i for i, n in enumerate(char_arr)}\n","\n","seq_data = [['man', 'woman'], ['men', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n","\n","# Seq2Seq Parameter\n","vocab_size = len(num_dic)\n","n_embed = 5\n","n_step = 5\n","n_hidden = 128\n","n_class = len(num_dic) # number of class(=number of vocab)\n","\n","def make_batch(seq_data):\n","    input_batch, output_batch, target_batch = [], [], []\n","\n","    for seq in seq_data:\n","        for i in range(2):\n","            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n","\n","        input = [num_dic[n] for n in seq[0]]\n","        output = [num_dic[n] for n in ('S' + seq[1])]\n","        target = [num_dic[n] for n in (seq[1] + 'E')]\n","\n","        input_batch.append(input)\n","        output_batch.append(output)\n","        target_batch.append(target)\n","\n","    return input_batch, output_batch, target_batch\n","\n","  \n","# Model\n","class seq2seq(object):\n","    def __init__(self, sess, vocab_size, n_embed, n_hidden, n_class):\n","        self.sess = sess\n","        self.n_hidden = n_hidden\n","        self.n_class = n_class\n","        self._build_model()\n","                \n","    def _build_model(self):\n","        self.enc_input = tf.placeholder(tf.int32, [None, n_step]) # [batch_size, max_len(=encoder_step), n_class]\n","        self.dec_input = tf.placeholder(tf.int32, [None, n_step+1]) # [batch_size, max_len+1(=decoder_step) (becase of 'S' or 'E'), n_class]\n","        self.targets = tf.placeholder(tf.int32, [None, None]) # [batch_size, max_len+1], not one-hot\n","        \n","        embedding = Embedding(vocab_size, n_embed)\n","        enc_input = embedding(self.enc_input)\n","        dec_input = embedding(self.dec_input)\n","        \n","        fc = Dense(n_class, input_shape=(n_hidden,))\n","\n","        with tf.variable_scope('encode'):\n","            enc_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n","            enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n","            _, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)\n","            # encoder state will go to decoder initial_state, enc_states : [batch_size, n_hidden(=128)]\n","\n","        with tf.variable_scope('decode'):\n","            dec_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n","            dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n","            outputs, _ = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state=enc_states, dtype=tf.float32)\n","\n","        logits = fc(outputs) # model : [batch_size, max_len+1, n_class]\n","\n","        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=self.targets))\n","        self.optimizer = tf.train.AdamOptimizer(0.001).minimize(self.cost)\n","\n","        self.prediction = tf.argmax(logits, 2)\n","\n","        sess.run(tf.global_variables_initializer())\n","    \n","    def train(self, enc_input, dec_input, targets):\n","        return self.sess.run([self.cost, self.optimizer], feed_dict={self.enc_input: enc_input, self.dec_input: dec_input, self.targets: targets})\n","     \n","    def predict(self, enc_input, dec_input):\n","        return self.sess.run(self.prediction, feed_dict={self.enc_input: enc_input, self.dec_input: dec_input})\n","                \n","\n","# Training\n","run_config = tf.ConfigProto()\n","run_config.gpu_options.allow_growth=True\n","sess =  tf.Session(config=run_config)\n","\n","input_batch, output_batch, target_batch = make_batch(seq_data)\n","model = seq2seq(sess, vocab_size, n_embed, n_hidden, n_class)\n","for epoch in range(1000):\n","    loss, _ = model.train(input_batch, output_batch, target_batch)\n","    if (epoch + 1) % 100 == 0:\n","        print('Epoch:', '%03d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","\n","# Test\n","def translate(word):\n","    seq_data = [word, 'P' * len(word)]\n","    \n","    input_batch, output_batch, _ = make_batch([seq_data])\n","    result = model.predict(input_batch, output_batch)\n","    decoded = [char_arr[i] for i in result[0]]\n","    if 'E' in decoded:\n","        end = decoded.index('E')\n","        translated = ''.join(decoded[:end])\n","    else:\n","        translated = ''.join(decoded)\n","    return translated.replace('P','')\n","\n","print('test')\n","print('man ->', translate('man'))\n","print('men ->', translate('men'))\n","print('king ->', translate('king'))\n","print('black ->', translate('black'))\n","print('upp ->', translate('upp'))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch: 100 cost = 1.567791\n","Epoch: 200 cost = 0.482214\n","Epoch: 300 cost = 0.179591\n","Epoch: 400 cost = 0.057933\n","Epoch: 500 cost = 0.041828\n","Epoch: 600 cost = 0.014552\n","Epoch: 700 cost = 0.013977\n","Epoch: 800 cost = 0.007216\n","Epoch: 900 cost = 0.016227\n","Epoch: 1000 cost = 0.015089\n","test\n","man -> woman\n","men -> women\n","king -> queen\n","black -> white\n","upp -> down\n"],"name":"stdout"}]}]}