{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"biLSTM-Tensor.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"yu0B_aUV-rPW","colab_type":"code","outputId":"cafe9281-7c1d-48d1-b700-755e3263cf18","executionInfo":{"status":"ok","timestamp":1550703670326,"user_tz":-540,"elapsed":51104,"user":{"displayName":"Minho Ryu","photoUrl":"https://lh5.googleusercontent.com/-DfVUFWgMLnc/AAAAAAAAAAI/AAAAAAAAApE/jyfqVt9TIvM/s64/photo.jpg","userId":"01830135931220651986"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"cell_type":"code","source":["'''\n","  code by Minho Ryu @bzantium\n","  \n","'''\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, Dense\n","import numpy as np\n","\n","tf.reset_default_graph()\n","\n","sentence = (\n","    'Lorem ipsum dolor sit amet consectetur adipisicing elit '\n","    'sed do eiusmod tempor incididunt ut labore et dolore magna '\n","    'aliqua Ut enim ad minim veniam quis nostrud exercitation'\n",")\n","\n","word_dict = {w: i for i, w in enumerate(list(set(sentence.split())))}\n","word_dict['<E>'] = len(word_dict)\n","number_dict = {i: w for w, i in word_dict.items()}\n","vocab_size = len(word_dict)\n","n_embed = 5\n","n_class = len(word_dict)\n","n_step = len(sentence.split())\n","n_hidden = 5\n","n_batch = 32\n","\n","def make_batch(sentence, n_batch):\n","    words = sentence.split()\n","    input_batch = []\n","    target_batch = []\n","    for _ in range(n_batch):\n","        input_batch.append([word_dict[n] for n in words])\n","        target_batch.append([word_dict[n] for n in (sentence + ' <E>').split()[1:]])\n","\n","    return input_batch, target_batch\n","\n","# Bi-LSTM Model\n","class biLSTM(object):\n","    def __init__(self, sess, vocab_size, n_embed, n_step, n_hidden, n_class):\n","        self.sess = sess\n","        self.vocab_size = vocab_size\n","        self.n_embed = n_embed\n","        self.n_step = n_step\n","        self.n_hidden = n_hidden\n","        self.n_class = n_class\n","        self._build_model()\n","        \n","    def _build_model(self):\n","        self.X = tf.placeholder(tf.int32, [None, n_step])\n","        self.Y = tf.placeholder(tf.int32, [None, n_step])\n","        embedding = Embedding(self.vocab_size, self.n_embed)\n","        inputs = embedding(self.X)\n","        \n","        fc = Dense(self.n_class, input_shape=(self.n_hidden * 2,))\n","\n","        lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(self.n_hidden)\n","        lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(self.n_hidden)\n","\n","        # outputs : [batch_size, len_seq, n_hidden], states : [batch_size, n_hidden]\n","        outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, inputs, dtype=tf.float32)\n","\n","        outputs = tf.concat([outputs[0], outputs[1]], -1) # output[0] : lstm_fw, output[1] : lstm_bw \n","        logits = fc(outputs) # [batch_size, n_step, 2 * n_hidden] -> [batch_size, n_step, n_class]\n","\n","        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=self.Y))\n","        self.optimizer = tf.train.AdamOptimizer(0.001).minimize(self.cost)\n","\n","        self.prediction = tf.cast(tf.argmax(logits, -1), tf.int32)\n","\n","        tf.global_variables_initializer().run()\n","    \n","    def train(self, inputs, labels):\n","        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: inputs, self.Y: labels})\n","     \n","    def predict(self, inputs):\n","        return self.sess.run(self.prediction, feed_dict={self.X: inputs})\n","\n","# Training\n","run_config = tf.ConfigProto()\n","run_config.gpu_options.allow_growth=True\n","with tf.Session(config=run_config) as sess:\n","    input_batch, target_batch = make_batch(sentence, n_batch)\n","    model = biLSTM(sess, vocab_size, n_embed, n_step, n_hidden, n_class)\n","    for epoch in range(1000):\n","        loss, _ = model.train(input_batch, target_batch)\n","        if (epoch + 1)%100 == 0:\n","            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","    predict =  model.predict([input_batch[0]])\n","    \n","print(sentence)\n","print([number_dict[n] for n in predict[0]])"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Epoch: 0100 cost = 3.078873\n","Epoch: 0200 cost = 2.383380\n","Epoch: 0300 cost = 1.902453\n","Epoch: 0400 cost = 1.554619\n","Epoch: 0500 cost = 1.287691\n","Epoch: 0600 cost = 1.070608\n","Epoch: 0700 cost = 0.872871\n","Epoch: 0800 cost = 0.686059\n","Epoch: 0900 cost = 0.526642\n","Epoch: 1000 cost = 0.402328\n","Lorem ipsum dolor sit amet consectetur adipisicing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua Ut enim ad minim veniam quis nostrud exercitation\n","['ipsum', 'dolor', 'sit', 'amet', 'consectetur', 'adipisicing', 'elit', 'sed', 'do', 'eiusmod', 'tempor', 'incididunt', 'ut', 'labore', 'et', 'dolore', 'magna', 'aliqua', 'Ut', 'enim', 'ad', 'minim', 'veniam', 'quis', 'nostrud', 'exercitation', '<E>']\n"],"name":"stdout"}]}]}